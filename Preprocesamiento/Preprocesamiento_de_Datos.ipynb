{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e3e15c",
   "metadata": {},
   "source": [
    "El presente código tiene por objetivo ejecutar el preprocesamiento de datos meteorológicos.\n",
    "\n",
    "Luego de importar la base de datos original, se eliminan las variables que no se considerarán en el proyecto; a continuación, se agregan filas correspondientes a los días no registrados en la base de datos. Se agregan columnas correspondientes a la latitud, longitud y altitud de las estaciones, así como una columna que indica el número de mes de cada registro.\n",
    "\n",
    "Se realiza un proceso de rellenado de datos faltantes. El proceso seguido para las diferentes variables meteorológicas es:\n",
    "\n",
    "1. Precipitación, temperatura máxima, media y mínima: Se sustituyen valores desconocidos por los proporcionados por la base de datos de ENACTS; los datos de ENACTS son importados de archivos en formato NetCDF con nombre 'precipitacion.nc', 'tmax.nc', 'tmed.nc', 'tmin.nc'.\n",
    "\n",
    "2. Humedad relativa, velocidad del viento y dirección del viento: Se realiza, para cada día desde el 1/1/1980 hasta el 31/5/2023, una interpolación geográfica utilizando la latitud y la longitud de las estaciones cuyo valor es conocido. Se utiliza Kriging a partir del paquete PyKrige; el método utilizado es interpolación lineal. Se evalúa, en la interpolación, la latitud y longitud de las estaciones con valor desconocido para hallar el valor a utilizar.\n",
    "\n",
    "3. Presión atmosférica: Se realiza, para cada día desde el 1/1/1980 hasta el 31/5/2023, una regresión lineal de presión atmosférica vs altitud con las cantidades de las estaciones de valor conocido. Se utiliza el paquete Scikit Learn para realizar la regresión. Se evalúa la altitud de las estaciones con valor desconocido para hallar el valor a utilizar.\n",
    "\n",
    "Seguidamente, se crea un dataframe con el listado de todos los días del año para cada una de las estaciones, que contiene la media, la desviación estándar, así como los percentiles P(1/3) y P(2/3) del conjunto de valores de precipitación de la Climatología. Este dataframe se exporta en un Excel hacia los archivos del usuario con nombre 'Percentiles.xlsx'. A partir de este dataframe, se agregan a la base de datos meteorológicos las variables objetivo:\n",
    "\n",
    "1. Anomalía absoluta de precipitación del día siguiente, respecto de la Climatología. (Variable continua)\n",
    "2. Clasificación de precipitación del día siguiente como =0 ó >0 (Variable categórica)\n",
    "3. Nivel cualitativo de precipitación del día siguiente en términos de los percentiles de la Climatología. (Variable categórica)\n",
    "\n",
    "Con respecto a la segunda variable:\n",
    "\n",
    "- Se ingresa 0 si la precipitación es =0\n",
    "- Se ingresa 1 si la precipitación es >0\n",
    "\n",
    "Con respecto a la tercera variable:\n",
    "\n",
    "- Se ingresa -1 si la precipitación es inferior o igual al percentil P(1/3).\n",
    "- Se ingresa +1 si la precipitación es superior o igual al percentil P(2/3).\n",
    "- Se ingresa 0 en otro caso.\n",
    "\n",
    "A continuación, se agrega columnas con las mediciones de las cantidades meteorológicas para cada uno de los últimos 7 días.\n",
    "\n",
    "Finalmente, se seleccionan las columnas que se desean en la base de datos final, y se exporta la misma hacia un archivo llamado 'BaseDatosProcesada.xlsx'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fb2558",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924a947",
   "metadata": {},
   "source": [
    "# IMPORTACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85522496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga la base de datos original\n",
    "\n",
    "import pandas as pd #Para manejo de dataframes\n",
    "import numpy as np #Para análisis de datos\n",
    "\n",
    "file_path = 'BaseDatos.xlsx'\n",
    "Datos = pd.read_excel(file_path)\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c156099",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b748a3",
   "metadata": {},
   "source": [
    "# ELIMINACIÓN DE VARIABLES A DESCARTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a3e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VarEliminar = ['Radiacion','Brillo_Solar','Evap_Tanque','Piche'] #Lista de variables a eliminar\n",
    "\n",
    "Datos.drop(columns=VarEliminar, inplace=True) #Eliminamos las columnas\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801eda22",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5c6b9",
   "metadata": {},
   "source": [
    "# AGREGAR DÍAS NO REGISTRADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2169d",
   "metadata": {},
   "source": [
    "EN PRIMER LUGAR, SE DEBE GENERAR UN REGISTRO DE LOS DÍAS QUE NO ESTÁN INGRESADOS, PARA CADA UNA DE LAS ESTACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame que contenga todas las fechas posibles en el rango de fechas para cada estación\n",
    "start_date = Datos['FECHA'].min()\n",
    "end_date = Datos['FECHA'].max()\n",
    "all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Crear una lista para almacenar los resultados\n",
    "missing_dates = []\n",
    "\n",
    "# Iterar sobre cada estación\n",
    "for estacion in Datos['ESTACIÓN'].unique():\n",
    "    # Filtrar los datos por estación\n",
    "    Datos_estacion = Datos[Datos['ESTACIÓN'] == estacion]\n",
    "    \n",
    "    # Obtener las fechas presentes en la base de datos para esta estación\n",
    "    fechas_present = Datos_estacion['FECHA']\n",
    "    \n",
    "    # Obtener las fechas faltantes para esta estación\n",
    "    fechas_missing = all_dates.difference(fechas_present)\n",
    "    \n",
    "    # Agregar las fechas faltantes a la lista de resultados\n",
    "    for date in fechas_missing:\n",
    "        missing_dates.append({'ESTACIÓN': estacion, 'FECHA': date})\n",
    "\n",
    "# Convertir la lista de resultados a un DataFrame\n",
    "missing_dates_Datos = pd.DataFrame(missing_dates)\n",
    "\n",
    "print(\"Las fechas no ingresadas en la base de datos son: \")\n",
    "print(missing_dates_Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea39a654",
   "metadata": {},
   "source": [
    "A CONTINUACIÓN, SE AGREGAN A LA BASE DE DATOS LOS DÍAS NO INGRESADOS; TODOS LOS VALORES DE ESTOS DÍAS APARECERAN VACÍOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7428e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añadir las columnas de datos meteorológicos con valores NaN\n",
    "for col in Datos.columns:\n",
    "    if col not in ['ESTACIÓN', 'FECHA']:\n",
    "        missing_dates_Datos[col] = np.nan\n",
    "\n",
    "# Combinar el DataFrame original con el DataFrame de fechas faltantes\n",
    "Datos = pd.concat([Datos, missing_dates_Datos], ignore_index=True)\n",
    "\n",
    "# Ordenar el DataFrame resultante por estación y fecha\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56efd786",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714fad4",
   "metadata": {},
   "source": [
    "# AGRAGAR LAT, LON, ALT, DÍA DEL MES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7211a6",
   "metadata": {},
   "source": [
    "A CONTINUACIÓN, SE AGREGA LA LATITUD, LONGITUD Y ALTURA DE LA ESTACIÓN, ASÍ COMO EL NÚMERO DE MES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la base de datos que contiene los nombres de las estaciones y su latitud, longitud y altura.\n",
    "file_path = 'Datos_estaciones.xlsx'\n",
    "Datos_estaciones = pd.read_excel(file_path)\n",
    "\n",
    "# Fusionar la base de datos con los datos de las estaciones, en función del nombre de la estación\n",
    "Datos = pd.merge(Datos, Datos_estaciones, on='ESTACIÓN', how='left')\n",
    "\n",
    "# Agregar una nueva columna con el número del mes\n",
    "Datos['MES'] = Datos['FECHA'].dt.month\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9510766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el avance en un dataframe\n",
    "Preliminar1 = Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac924399",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1beb56e",
   "metadata": {},
   "source": [
    "# RELLENO DE DATOS FALTANTES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844045d",
   "metadata": {},
   "source": [
    "### LLUVIA, TMAX, TMED, TMIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b872975",
   "metadata": {},
   "source": [
    "EN PRIMER LUGAR, SE RELIZA EL RELLENO DE DATOS FALTANTES DE LLUVIA Y TEMPERATURA MEDIA, MÁXIMA Y MÍNIMA, POR MEDIO DE LOS DATOS DE ENACTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8263cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cuántos valores son desconocidos\n",
    "print(\"Valores vaciós de precipitación: \" + str(Datos['Lluvia'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura media: \" + str(Datos['Tmed'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura máxima: \" + str(Datos['Tmax'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura mínima: \" + str(Datos['Tmin'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccfec71-9ef5-4933-9c3c-03dd6d9fc9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Suprimir advertencias de tipo FutureWarning\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Crear una copia del DataFrame original, la cual se irá rellenando\n",
    "Datos_rellenados = Datos.copy()\n",
    "\n",
    "# Mapeo de variables de la base de datos a las variables en los archivos NetCDF (esto se hace ya que el nombre de la variable\n",
    "# en la base de datos meteorológicos puede no tener el mismo nombre que la variable en el archivo NetCDF)\n",
    "var_map = {\n",
    "    'Lluvia': 'rfe',\n",
    "    'Tmin': 'tmin',\n",
    "    'Tmax': 'tmax',\n",
    "    'Tmed': 'tmed'\n",
    "}\n",
    "\n",
    "# Archivos NetCDF correspondientes\n",
    "nc_files = {\n",
    "    'rfe': 'precipitacion.nc',\n",
    "    'tmin': 'tmin.nc',\n",
    "    'tmax': 'tmax.nc',\n",
    "    'tmed': 'tmed.nc'\n",
    "}\n",
    "\n",
    "# Cargar los archivos NetCDF en un diccionario\n",
    "nc_data = {var: xr.open_dataset(file) for var, file in nc_files.items()}\n",
    "\n",
    "# Iterar sobre las filas\n",
    "for i, row in Datos.iterrows():\n",
    "    # Extraer latitud, longitud y fecha para la fila actual\n",
    "    lat = row['Latitud']\n",
    "    lon = row['Longitud']\n",
    "    fecha = row['FECHA']\n",
    "    estacion = row['ESTACIÓN']\n",
    "    \n",
    "    # Iterar sobre las variables para la fila actual\n",
    "    for db_var, nc_var in var_map.items():\n",
    "        if pd.isna(row[db_var]):\n",
    "            # Extraer los datos de la variable correspondiente del archivo NetCDF\n",
    "            valor = nc_data[nc_var][nc_var].sel(\n",
    "                Y=lat, X=lon, T=fecha, method='nearest'\n",
    "            ).values\n",
    "            \n",
    "            # Reemplazar el valor en el DataFrame\n",
    "            Datos_rellenados.at[i, db_var] = valor\n",
    "            \n",
    "    print(\"Se han rellenado las variables en la estación \" + \"'\" + estacion + \"'\" + \" en la fecha \" + str(fecha))\n",
    "\n",
    "print(Datos_rellenados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbc576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar el DataFrame original con los datos rellenados\n",
    "Datos = Datos_rellenados\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que no quede ningún valor vacío\n",
    "print(\"Valores vaciós de precipitación: \" + str(Datos['Lluvia'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura media: \" + str(Datos['Tmed'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura máxima: \" + str(Datos['Tmax'].isna().sum()))\n",
    "print(\"Valores vaciós de temperatura mínima: \" + str(Datos['Tmin'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767ab39",
   "metadata": {},
   "source": [
    "### HUMEDAD_R, VEL_VIENTO, DIR_VIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f828f",
   "metadata": {},
   "source": [
    "A CONTINUACIÓN, SE RELLENAN DATOS FALTANTES DE HUMEDAD, VELOCIDAD Y DIRECCIÓN DEL VIENTO MEDIANTE UNA INTERPOLACIÓN GEOMÉTRICA PARA CADA DÍA, UTILIZANDO LA METODOLOGÍA 'KRIGING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ae846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cuántos valores son desconocidos\n",
    "print(\"Valores vaciós de humedad relativa: \" + str(Datos['Humedad_R'].isna().sum()))\n",
    "print(\"Valores vaciós de velocidad del viento: \" + str(Datos['Vel_viento'].isna().sum()))\n",
    "print(\"Valores vacíos de dirección del viento: \" + str(Datos['Dir_viento'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4dfd2-45cf-4a02-85ea-d68dea684ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykrige.ok import OrdinaryKriging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Lista de variables a procesar mediante interpolación geográfica\n",
    "variables = ['Humedad_R', 'Vel_viento', 'Dir_viento']\n",
    "\n",
    "# Inicializa un DataFrame para almacenar los resultados\n",
    "Datos_rellenados = pd.DataFrame()\n",
    "\n",
    "# Obtiene la lista de fechas únicas\n",
    "fechas_unicas = Datos['FECHA'].unique()\n",
    "\n",
    "for fecha in fechas_unicas:\n",
    "    # Filtra los datos para la fecha actual\n",
    "    df_dia = Datos[Datos['FECHA'] == fecha]\n",
    "    \n",
    "    for variable in variables:\n",
    "        \n",
    "        # Agregar un mensaje que permita conocer el avance del proceso\n",
    "        print(\"Se ha rellenado la variable \" + \"'\" + variable + \"' \" + \"en la fecha \" + str(fecha))\n",
    "        \n",
    "        # Filtra los datos conocidos (no nulos) para la variable actual\n",
    "        known_data = df_dia.dropna(subset=[variable])\n",
    "        \n",
    "        # Si no hay datos conocidos, salta la interpolación\n",
    "        if known_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extrae las coordenadas y valores conocidos\n",
    "        known_coords = known_data[['Latitud', 'Longitud']].values\n",
    "        known_var = known_data[variable].values\n",
    "        \n",
    "        # Verifica si todos los valores de la variable conocida son cero\n",
    "        if np.all(known_var == 0):\n",
    "            df_dia.loc[df_dia[variable].isnull(), variable] = 0\n",
    "            continue\n",
    "        \n",
    "        # Filtra los datos desconocidos (nulos)\n",
    "        unknown_data = df_dia[df_dia[variable].isnull()]\n",
    "        \n",
    "        # Si no hay datos desconocidos, no se necesita interpolación\n",
    "        if unknown_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extrae las coordenadas desconocidas\n",
    "        unknown_coords = unknown_data[['Latitud', 'Longitud']].values\n",
    "        \n",
    "        # Realiza la interpolación utilizando Ordinary Kriging\n",
    "        OK = OrdinaryKriging(\n",
    "            known_coords[:, 1], known_coords[:, 0], known_var,\n",
    "            variogram_model='linear', verbose=False, enable_plotting=False\n",
    "        )\n",
    "        \n",
    "        # Interpola los valores para las coordenadas desconocidas\n",
    "        interpolated_var, ss = OK.execute('points', unknown_coords[:, 1], unknown_coords[:, 0])\n",
    "        \n",
    "        #Reemplazar posibles valores negativos por cero\n",
    "        interpolated_var[interpolated_var < 0] = 0\n",
    "        \n",
    "        # Reemplaza los valores nulos en el DataFrame original\n",
    "        df_dia.loc[df_dia[variable].isnull(), variable] = interpolated_var\n",
    "    \n",
    "    # Concatenar los resultados al DataFrame resultante\n",
    "    Datos_rellenados = pd.concat([Datos_rellenados, df_dia])\n",
    "\n",
    "print(Datos_rellenados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar el DataFrame original con los datos rellenados\n",
    "Datos = Datos_rellenados\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar que no quede ningún valor vacío\n",
    "print(\"Valores vaciós de humedad relativa: \" + str(Datos['Humedad_R'].isna().sum()))\n",
    "print(\"Valores vaciós de velocidad del viento: \" + str(Datos['Vel_viento'].isna().sum()))\n",
    "print(\"Valores vacíos de dirección del viento: \" + str(Datos['Dir_viento'].isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de829f",
   "metadata": {},
   "source": [
    "### PRESION_ATMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07940f1a",
   "metadata": {},
   "source": [
    "AHORA, SE RELIZA EL RELLENO DE DATOS FALTANTES DE PRESIÓN ATMOSFÉRICA, A TRAVÉS DE UNA REGRESIÓN LINEAL CON LOS VALORES DE PRESIÓN VS ALTURA PARA LAS ESTACIONES CON VALOR CONOCIDO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65320f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cuántos valores son desconocidos\n",
    "print(\"Valores vaciós de presión atmosférica: \" + str(Datos['Presion_atms'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d4174",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Lista de variables de temperatura a procesar\n",
    "variables = ['Presion_atms']\n",
    "\n",
    "# Inicializa un DataFrame para almacenar los resultados\n",
    "Datos_rellenados = pd.DataFrame()\n",
    "\n",
    "# Obtiene la lista de fechas únicas\n",
    "fechas_unicas = Datos['FECHA'].unique()\n",
    "\n",
    "for fecha in fechas_unicas:\n",
    "    # Filtra los datos para la fecha actual\n",
    "    df_dia = Datos[Datos['FECHA'] == fecha]\n",
    "    \n",
    "    for variable in variables:\n",
    "        \n",
    "        # Agregar un mensaje que permita conocer el avance del proceso\n",
    "        print(\"Se ha rellenado la variable \" + \"'\" + variable + \"' \" + \"en la fecha \" + str(fecha))\n",
    "        \n",
    "        # Filtra los datos conocidos (no nulos) para la variable actual\n",
    "        known_data = df_dia.dropna(subset=[variable])\n",
    "        \n",
    "        # Si no hay datos conocidos, salta la interpolación\n",
    "        if known_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extrae las coordenadas de altitud y valores conocidos\n",
    "        known_altitude = known_data[['Altitud']].values\n",
    "        known_atm = known_data[variable].values\n",
    "        \n",
    "        # Filtra los datos desconocidos (nulos)\n",
    "        unknown_data = df_dia[df_dia[variable].isnull()]\n",
    "        \n",
    "        # Si no hay datos desconocidos, no se necesita interpolación\n",
    "        if unknown_data.empty:\n",
    "            continue\n",
    "        \n",
    "        # Extrae las altitudes desconocidas\n",
    "        unknown_altitude = unknown_data[['Altitud']].values\n",
    "        \n",
    "        # Realiza la regresión lineal para la variable actual\n",
    "        model = LinearRegression()\n",
    "        model.fit(known_altitude, known_atm)\n",
    "        \n",
    "        # Predice los valores para las altitudes desconocidas\n",
    "        predicted_atm = model.predict(unknown_altitude)\n",
    "        \n",
    "        # Reemplazar posibles valores negativos por cero\n",
    "        predicted_atm[predicted_atm < 0] = 0\n",
    "        \n",
    "        # Reemplaza los valores nulos en el DataFrame original\n",
    "        df_dia.loc[df_dia[variable].isnull(), variable] = predicted_atm\n",
    "    \n",
    "    # Concatenar los resultados al DataFrame resultante\n",
    "    Datos_rellenados = pd.concat([Datos_rellenados, df_dia])\n",
    "\n",
    "print(Datos_rellenados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualizar el DataFrame original con los datos rellenados\n",
    "Datos = Datos_rellenados\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c87b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar cuántos valores quedan vacíos\n",
    "print(\"Valores vaciós de presión atmosférica: \" + str(Datos['Presion_atms'].isna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, guardamos el avance en un dataframe\n",
    "Preliminar2 = Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da0e37a",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd82e7eb",
   "metadata": {},
   "source": [
    "# AGREGAR VARIABLE OBJETIVO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48536dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar los datos por fecha y estación\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "# Crear una columna de \"día del año\" para posteriormente poder agrupar los datos por día\n",
    "Datos['DiaDelAno'] = Datos['FECHA'].dt.strftime('%m-%d')\n",
    "\n",
    "# Filtrar los datos de la Climatología (1991 - 2020)\n",
    "Datos_Climatologia = Datos[(Datos['FECHA'].dt.year >= 1991) & (Datos['FECHA'].dt.year <= 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fca676",
   "metadata": {},
   "source": [
    "### DATOS CLIMATOLOGÍA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ffb23",
   "metadata": {},
   "source": [
    "EN PRIMER LUGAR, SE GENERA UN DATAFRAME CON EL LISTADO DE TODAS LAS ESTACIONES, PARA CADA DÍA DEL AÑO (366), INDICANDO LA MEDIA Y DESVIACIÓN ESTANDAR DEL CONJUNTO DE PRECIPITACIONES DE LA CLIMATOLOGÍA PARA DICHO DÍA, ASÍ COMO EL VALOR DEL PERCENTIL 1/3 Y EL PERCENTIL 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558d5b31-e7b5-462d-870b-af7fbc53806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataframe para almacenar los percentiles\n",
    "percentiles_df = pd.DataFrame(columns=['ESTACIÓN', 'DiaDelAno', 'Media','Desv_est', 'Percentil_1_/3', 'Percentil_2/3'])\n",
    "\n",
    "# Obtener las estaciones únicas\n",
    "estaciones_unicas = Datos['ESTACIÓN'].unique()\n",
    "\n",
    "# Calcular los percentiles 1/3 y 2/3 para cada estación y día del año\n",
    "for estacion in estaciones_unicas:\n",
    "    for dia in Datos['DiaDelAno'].unique():  # Iterar sobre cada día del año en formato mm-dd\n",
    "        # Filtrar los datos por estación y día del año\n",
    "        datos_dia_estacion = Datos_Climatologia[(Datos_Climatologia['ESTACIÓN'] == estacion) & \n",
    "                                             (Datos_Climatologia['DiaDelAno'] == dia)]['Lluvia']\n",
    "        \n",
    "        # Calcular los percentiles\n",
    "        percentil_1_3 = np.percentile(datos_dia_estacion, 100/3)\n",
    "        percentil_2_3 = np.percentile(datos_dia_estacion, 200/3)\n",
    "        promedio = datos_dia_estacion.mean()\n",
    "        desviacion_estandar = datos_dia_estacion.std()\n",
    "\n",
    "        # Agregar los resultados al dataframe\n",
    "        percentiles_df = percentiles_df.append({'ESTACIÓN': estacion,\n",
    "                                                'DiaDelAno': dia,\n",
    "                                                'Media': promedio,\n",
    "                                                'Desv_est': desviacion_estandar,\n",
    "                                                'Percentil_1_/3': percentil_1_3,\n",
    "                                                'Percentil_2/3': percentil_2_3},\n",
    "                                                ignore_index=True)\n",
    "        \n",
    "        # Agregar mensaje que indique el avance del proceso\n",
    "        print(\"Se ha realizado el proceso para la estación \" + estacion + \" para el día \" + dia)\n",
    "\n",
    "# Guardar el DataFrame con los percentiles de cada día para la Climatología un nuevo archivo Excel\n",
    "output_file_path = 'Percentiles.xlsx'\n",
    "percentiles_df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(\"El proceso ha culminado, los resultados han sido exportados en un archivo con nombre: \" + output_file_path)\n",
    "\n",
    "# Mostrar el dataframe con los percentiles calculados\n",
    "print(percentiles_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ea9fd",
   "metadata": {},
   "source": [
    "### VARIABLE OBJETIVO 1 (CONTINUA) (PARA MODELOS DE REGRESIÓN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f52418",
   "metadata": {},
   "source": [
    "SE AGREGA UNA COLUMNA A LA BASE DE DATOS CON LA PRIMERA VARIABLE OBJETIVO: LA ANOMALÍA ABSOLUTA DE PRECIPITACIÓN DEL DÍA SIGUIENTE, RESPECTO DE LA CLIMATOLOGÍA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un merge de los datos meteorológicos con los datos de la Climatología\n",
    "Datos = Datos.merge(percentiles_df, on=['ESTACIÓN', 'DiaDelAno'], how='left')\n",
    "\n",
    "# Creamos la nueva columna de anomalía absoluta\n",
    "Datos['Anom_prec_mañana'] = Datos['Lluvia'] - Datos['Media']\n",
    "\n",
    "# Eliminar las columnas \"Desv_est\", \"Percentil_1_/3\" y \"Percentil_2/3\" ya que no son necesaria en la base de datos meteorológicos\n",
    "Datos = Datos.drop(columns=['Desv_est','Percentil_1_/3','Percentil_2/3'])\n",
    "\n",
    "# Ordenar los datos por fecha y estación\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "# Desplazamos la columna 'ANOMALIA_PRECIPITACION' una fila hacia atrás ya que queremos el dato del día siguiente\n",
    "Datos['Anom_prec_mañana'] = Datos.groupby('ESTACIÓN')['Anom_prec_mañana'].shift(-1)\n",
    "\n",
    "# Desplazamos la columna \"Media\" una fila hacia atrás ya que queremos la media de la precipitación del día siguiente\n",
    "Datos['Media'] = Datos.groupby('ESTACIÓN')['Media'].shift(-1)\n",
    "\n",
    "# Renombramos la columna \"Media\" por \"Media_prec_mañana_Clim\"\n",
    "Datos.rename(columns={'Media': 'Media_prec_mañana_Clim'}, inplace=True)\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34829c7f",
   "metadata": {},
   "source": [
    "### VARIABLE OBJETIVO 2 (CATEGÓRICA) (PARA MODELOS DE CLASIFICACIÓN) (LLUEVE SÍ/NO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c5e5cd",
   "metadata": {},
   "source": [
    "SE AGREGA UNA COLUMNA CON LA VARIABLE OBJETIVO: CLASIFICACIÓN DE PRECIPITACIÓN DEL DÍA SIGUIENTE COMO =0 (NO LLUEVE) ó >0 (SÍ LLUEVE)\n",
    "\n",
    "Si la lluvia del dia siguiente, en comparación con la Climatología, es:\n",
    "- $=0mm$, se ingresa 0 \n",
    "- $>0mm$, se ingresa +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3960d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una función que nos devuelva 0 si no llueve y 1 si sí llueve\n",
    "def Lluvia_Binaria(lluvia):\n",
    "    if lluvia == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Creamos la nueva columna de clasificación binaria de la lluvia usando la función previamente definida\n",
    "Datos['Lluvia_Binaria'] = Datos.apply(\n",
    "    lambda row: Lluvia_Binaria(row['Lluvia']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ordenar los datos por fecha y estación\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "# Desplazar la columna una fila hacia atrás ya que queremos el dato del día de mañana\n",
    "Datos['Lluvia_Binaria'] = Datos.groupby('ESTACIÓN')['Lluvia_Binaria'].shift(-1)\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b5039b",
   "metadata": {},
   "source": [
    "### VARIABLE OBJETIVO 3 (CATEGÓRICA) (PARA MODELOS DE CLASIFICACIÓN) (COTAS PERCENTILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d2de9",
   "metadata": {},
   "source": [
    "SE AGREGA UNA COLUMNA CON LA VARIABLE OBJETIVO: NIVEL CUALITATIVO DE LA PRECIPITACIÓN DEL DÍA SIGUIENTE EN TÉRMINOS DE LOS PERCENTILES DE LA CLIMATOLOGÍA\n",
    "\n",
    "Si la lluvia del dia siguiente, en comparación con la Climatología, es:\n",
    "- Inferior o igual al P(1/3), se ingresa -1 \n",
    "- Superior o igual al P(2/3), se ingresa +1\n",
    "- Mayor al P(1/3) y menor al P(2/3), se intresa 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5eae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos un merge de los datos meteorológicos con los datos de la Climatología\n",
    "Datos = Datos.merge(percentiles_df, on=['ESTACIÓN', 'DiaDelAno'], how='left')\n",
    "\n",
    "# Crear la columna que indique el percentil en el que se encuentra la lluvia del día siguiente\n",
    "def percentil(precipitacion, Percentil_un_tercio, Percentil_dos_tercios):\n",
    "    if Percentil_un_tercio == 0 and Percentil_dos_tercios == 0 and precipitacion == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        if precipitacion <= Percentil_un_tercio:\n",
    "            return -1\n",
    "        elif precipitacion >= Percentil_dos_tercios:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "Datos['percentil'] = Datos.apply(\n",
    "    lambda row: percentil(row['Lluvia'], row['Percentil_1_/3'], row['Percentil_2/3']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ordenar los datos por fecha y estación\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "# Desplazar la columna de comportamiento cualitativo una fila hacia atrás ya que queremos el dato del día de mañana\n",
    "Datos['percentil'] = Datos.groupby('ESTACIÓN')['percentil'].shift(-1)\n",
    "\n",
    "# Eliminar las columnas \"media\", \"Desv_est\", \"Percentil_1_/3\" y \"Percentil_2/3\" ya que no son necesaria en la base de datos meteorológicos\n",
    "Datos = Datos.drop(columns=['Media','Desv_est','Percentil_1_/3','Percentil_2/3'])\n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3750d835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, guardamos el avance en un dataframe\n",
    "Preliminar3 = Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e655f4b",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb575a3",
   "metadata": {},
   "source": [
    "# GENERAR UN DATAFRAME CON LOS VALORES DE LOS ÚLTIMOS 7 DÍAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f677b2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar los datos por fecha y estación\n",
    "Datos = Datos.sort_values(by=['ESTACIÓN', 'FECHA'])\n",
    "\n",
    "# Crear nuevas columnas para las variables de los 7 días anteriores\n",
    "for i in range(1, 8):\n",
    "    Datos[f'Lluvia_{i}d'] = Datos.groupby('ESTACIÓN')['Lluvia'].shift(i)\n",
    "    Datos[f'Humedad_R_{i}d'] = Datos.groupby('ESTACIÓN')['Humedad_R'].shift(i)\n",
    "    Datos[f'Tmin_{i}d'] = Datos.groupby('ESTACIÓN')['Tmin'].shift(i)\n",
    "    Datos[f'Tmax_{i}d'] = Datos.groupby('ESTACIÓN')['Tmax'].shift(i)\n",
    "    Datos[f'Tmed_{i}d'] = Datos.groupby('ESTACIÓN')['Tmed'].shift(i)\n",
    "    Datos[f'Presion_atms_{i}d'] = Datos.groupby('ESTACIÓN')['Presion_atms'].shift(i)\n",
    "    Datos[f'Vel_viento_{i}d'] = Datos.groupby('ESTACIÓN')['Vel_viento'].shift(i)\n",
    "    Datos[f'Dir_viento_{i}d'] = Datos.groupby('ESTACIÓN')['Dir_viento'].shift(i) \n",
    "\n",
    "print(Datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores vacíos debido a los shift's\n",
    "Datos = Datos.dropna()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32daf3f8",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84e885b",
   "metadata": {},
   "source": [
    "# SELECCIÓN FINAL DE LA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77361fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas que queremos en la base de datos final\n",
    "columns = ['Latitud', 'Longitud', 'MES'] + [f'{var}_{i}d' for var in ['Lluvia', 'Humedad_R', 'Tmin', 'Tmax', 'Tmed', 'Vel_viento', 'Dir_viento', 'Presion_atms'] for i in range(1, 8)] + ['Anom_prec_mañana', 'Lluvia_Binaria', 'percentil'] + ['Media_prec_mañana_Clim']\n",
    "DatosFinales = Datos[columns]\n",
    "\n",
    "# Reiniciar el índice del DataFrame resultante\n",
    "DatosFinales.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(DatosFinales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81220ec7",
   "metadata": {},
   "source": [
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872357bb",
   "metadata": {},
   "source": [
    "# EXPORTACIÓN DE LA NUEVA BASE DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame resultante en un nuevo archivo Excel\n",
    "output_file_path = 'BaseDatosProcesada.xlsx'\n",
    "DatosFinales.to_excel(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
